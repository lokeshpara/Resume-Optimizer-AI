# CIFAR-10 Image Classification with Custom ResNet Architecture

## üìã Project Overview

This project implements a custom ResNet architecture for CIFAR-10 image classification, achieving **92.22% test accuracy** in just 24 epochs. The project demonstrates advanced deep learning techniques including One Cycle Policy learning rate scheduling, modern data augmentation with Albumentations, and comprehensive model evaluation.

---

## üéØ What I Did - Step-by-Step Process

### 1. **Data Preparation and Augmentation**
- **Downloaded CIFAR-10 dataset** (50,000 training images, 10,000 test images, 10 classes)
- **Implemented custom Albumentations pipeline** for advanced data augmentation:
  - PadIfNeeded (40x40 with reflection padding)
  - RandomCrop (32x32)
  - HorizontalFlip (probability 0.5)
  - Normalization (ImageNet statistics)
  - CutOut (1 hole, 16x16 size)
- **Created separate data loaders** for training (with augmentation) and testing (normalization only)
- **Configured batch size of 512** with 4 workers and pin memory for efficient GPU utilization

### 2. **Model Architecture Design**
- **Designed custom ResNet-inspired architecture** (`A11.py`) with:
  - PrepLayer: 3x3 Conv ‚Üí BatchNorm ‚Üí ReLU (64 channels)
  - Layer 1: 3x3 Conv ‚Üí MaxPool ‚Üí BatchNorm ‚Üí ReLU (128 channels) + ResBlock 1 with skip connection
  - Layer 2: 3x3 Conv ‚Üí MaxPool ‚Üí BatchNorm ‚Üí ReLU (256 channels)
  - Layer 3: 3x3 Conv ‚Üí MaxPool ‚Üí BatchNorm ‚Üí ReLU (512 channels) + ResBlock 2 with skip connection
  - Final: MaxPool(4) ‚Üí Fully Connected (10 classes) ‚Üí Log SoftMax
- **Implemented residual blocks** with skip connections to enable gradient flow
- **Calculated receptive fields** for each layer (ranging from 3x3 to 88x88)

### 3. **Learning Rate Range Test**
- **Conducted LR range test** from 0.0001 to 0.01 over 10 epochs
- **Observed accuracy progression**: Started at 21.20% (LR=0.0001) and peaked at 41.04% (LR=0.00406)
- **Identified optimal learning rate range** for One Cycle Policy
- **Visualized results** with LR vs Accuracy plot

### 4. **Training with One Cycle Policy**
- **Configured OneCycleLR scheduler** with:
  - Max LR: 0.04006 (peak at epoch 5)
  - Epochs: 24
  - Percentage start: 0.208 (20.8% of training for warmup)
  - Linear annealing strategy
  - Base momentum: 0.85, Max momentum: 0.95
  - Division factor: 10.0
- **Used SGD optimizer** with:
  - Initial LR: 0.01
  - Momentum: 0.9
  - Weight decay: 0.005
- **Trained for 24 epochs** with progress tracking using tqdm

### 5. **Model Evaluation**
- **Tested model after each epoch** to track performance
- **Collected misclassified and correctly classified images** in the final epoch
- **Performed class-wise accuracy analysis** for all 10 CIFAR-10 classes
- **Generated visualizations** for training/test accuracy curves and learning rate schedules

---

## üìä Results & Impact

### **Final Performance Metrics**

| Metric | Value |
|-------|-------|
| **Best Test Accuracy** | **92.22%** (Epoch 23) |
| **Final Training Accuracy** | **93.89%** (Epoch 23) |
| **Final Test Loss** | 0.0004 |
| **Total Epochs** | 24 |
| **Training Time** | ~23 seconds per epoch |

### **Training Progression**

The model showed consistent improvement throughout training:

| Epoch | Learning Rate | Train Accuracy | Test Accuracy | Test Loss |
|-------|---------------|----------------|---------------|----------|
| 0 | 0.004006 | 40.78% | 52.10% | 0.0026 |
| 5 | 0.038148 | 75.51% | 77.53% | 0.0013 |
| 10 | 0.028664 | 83.65% | 84.36% | 0.0009 |
| 15 | 0.019180 | 87.43% | 87.71% | 0.0007 |
| 20 | 0.009696 | 91.17% | 89.19% | 0.0006 |
| **23** | **0.004006** | **93.89%** | **92.22%** | **0.0004** |

### **Class-wise Performance Analysis**

Detailed accuracy breakdown for each CIFAR-10 class:

| Class | Accuracy | Notes |
|-------|----------|-------|
| **Plane** | **100%** | Perfect classification |
| **Car** | **100%** | Perfect classification |
| **Bird** | **100%** | Perfect classification |
| **Cat** | **100%** | Perfect classification |
| **Deer** | **100%** | Perfect classification |
| **Dog** | **87%** | Lower due to similarity with cat |
| **Frog** | **100%** | Perfect classification |
| **Horse** | **100%** | Perfect classification |
| **Ship** | **100%** | Perfect classification |
| **Truck** | **83%** | Lower due to similarity with car |

**Key Insights:**
- 8 out of 10 classes achieved **100% accuracy**
- Only dog (87%) and truck (83%) showed lower performance due to visual similarity with cat and car respectively
- Overall model demonstrates excellent generalization across diverse object categories

### **Learning Rate Range Test Results**

| Epoch | Learning Rate | Training Accuracy | Loss |
|-------|---------------|------------------|------|
| 1 | 0.0001 | 21.20% | 1.965 |
| 2 | 0.00109 | 36.26% | 1.570 |
| 3 | 0.00208 | 40.11% | 1.339 |
| 4 | 0.00307 | 40.94% | 1.294 |
| **5** | **0.00406** | **41.04%** | **1.372** |
| 6 | 0.00505 | 40.68% | 1.276 |
| 7 | 0.00604 | 35.17% | 1.359 |
| 8 | 0.00703 | 24.54% | 1.686 |
| 9 | 0.00802 | 26.54% | 1.676 |
| 10 | 0.00901 | 21.97% | 1.801 |

**Finding:** Optimal learning rate range identified around 0.00406, which informed the One Cycle Policy configuration.

### **Impact of Techniques Used**

1. **One Cycle Policy**: Enabled rapid convergence from 52% to 92% accuracy in 24 epochs
2. **Residual Connections**: Allowed training of deeper network without vanishing gradient problems
3. **Advanced Augmentation**: Improved generalization, reducing overfitting gap (93.89% train vs 92.22% test)
4. **Batch Normalization**: Stabilized training and enabled higher learning rates
5. **Weight Decay**: Regularized the model, preventing overfitting

---

## üõ†Ô∏è Tools & Features Used

### **Core Deep Learning Frameworks**

1. **PyTorch** (`torch`)
   - Neural network architecture implementation
   - Automatic differentiation and backpropagation
   - GPU acceleration with CUDA support
   - Tensor operations and data manipulation

2. **TorchVision** (`torchvision`)
   - CIFAR-10 dataset loading
   - Built-in data transforms
   - Pre-trained model utilities

### **Data Augmentation Library**

3. **Albumentations** (`albumentations`)
   - Advanced image augmentation pipeline
   - **Features used:**
     - `PadIfNeeded`: Reflection padding to 40x40
     - `RandomCrop`: Random cropping to 32x32
     - `HorizontalFlip`: Random horizontal flipping (p=0.5)
     - `Normalize`: ImageNet statistics normalization
     - `CutOut`: Random cutout augmentation (1 hole, 16x16)
   - `ToTensor`: Conversion to PyTorch tensors
   - OpenCV backend for efficient image processing

### **Optimization & Scheduling**

4. **PyTorch Optimizers** (`torch.optim`)
   - **SGD Optimizer** with:
     - Momentum: 0.9 (for faster convergence)
     - Weight decay: 0.005 (L2 regularization)
     - Initial learning rate: 0.01

5. **OneCycleLR Scheduler** (`torch.optim.lr_scheduler.OneCycleLR`)
   - **Configuration:**
     - Max learning rate: 0.04006
     - Total epochs: 24
     - Steps per epoch: 1
     - Percentage start: 0.208 (warmup phase)
     - Anneal strategy: Linear
     - Cycle momentum: Disabled
     - Base momentum: 0.85
     - Max momentum: 0.95
     - Division factor: 10.0
     - Final division factor: 1.0

### **Loss Functions**

6. **CrossEntropyLoss** (`torch.nn.CrossEntropyLoss`)
   - Multi-class classification loss
   - Combines LogSoftmax and NLLLoss
   - Compatible with model's log_softmax output

### **Progress Tracking & Visualization**

7. **tqdm** (`tqdm`)
   - Real-time progress bars during training
   - Batch-level and epoch-level progress tracking
   - Time estimates and speed metrics

8. **Matplotlib** (`matplotlib`)
   - **Visualizations created:**
     - Learning rate vs accuracy plot
     - Learning rate vs epochs plot (One Cycle Policy)
     - Training and test accuracy curves
     - Cyclic learning rate schedule visualization

### **Numerical Computing**

9. **NumPy** (`numpy`)
   - Array operations for image preprocessing
   - Numerical computations
   - Data type conversions

10. **OpenCV** (`cv2`)
    - Image processing backend for Albumentations
    - Border reflection for padding
    - Efficient image transformations

### **Model Analysis Tools**

11. **torchsummary** (`torchsummary`)
    - Model architecture summary
    - Parameter counting
    - Layer-wise output shape visualization

### **Development Environment**

12. **Jupyter Notebook** (`EVA_session11_assignment.ipynb`)
    - Interactive development and experimentation
    - Step-by-step execution and debugging
    - Inline visualization and results display

13. **Google Colab**
    - Cloud-based GPU acceleration
    - Free GPU resources (CUDA)
    - Integrated development environment

### **Custom Modules Developed**

14. **dataloader11.py**
    - Custom Albumentation class implementation
    - CIFAR-10 dataset preparation
    - DataLoader configuration with multiprocessing

15. **train11.py**
    - Training loop implementation
    - Real-time accuracy and loss tracking
    - Progress bar integration
    - Metric storage for visualization

16. **test11.py**
    - Model evaluation functionality
    - Test accuracy and loss calculation
    - Misclassification collection
    - Class-wise performance analysis

17. **lr_range_test.py**
    - Learning rate range finder implementation
    - LR sweep from min to max
    - Accuracy tracking at different learning rates
    - Results storage for plotting

18. **models/A11.py**
    - Custom ResNet architecture
    - Residual block implementation
    - Skip connection design

### **Key Features & Techniques Implemented**

1. **Residual Connections (Skip Connections)**
   - Enables gradient flow through deep networks
   - Prevents vanishing gradient problem
   - Allows training of deeper architectures

2. **Batch Normalization**
   - Normalizes layer inputs
   - Stabilizes training
   - Enables higher learning rates
   - Reduces internal covariate shift

3. **One Cycle Policy**
   - Super-convergence technique
   - Faster training with fewer epochs
   - Better generalization
   - Optimal learning rate discovery

4. **Advanced Data Augmentation**
   - Increases dataset diversity
   - Reduces overfitting
   - Improves model robustness
   - Better generalization to unseen data

5. **Weight Decay Regularization**
   - L2 regularization
   - Prevents overfitting
   - Encourages smaller weights
   - Improves generalization

6. **Momentum in SGD**
   - Accelerates convergence
   - Smoothens gradient updates
   - Helps escape local minima
   - Faster training

---

## üìà Visualizations Generated

The project includes comprehensive visualizations stored in the `images/` folder:

1. **LR vs Accuracy Plot** (`LR plot.png`)
   - Shows relationship between learning rate and training accuracy
   - Identifies optimal learning rate range
   - Generated from LR range test results

2. **Cyclic LR Plot** (`cyclic LR plot.png`)
   - Demonstrates triangular cyclic learning rate schedule
   - Shows warmup, peak, and annealing phases
   - Visualizes One Cycle Policy implementation

3. **Train & Test Accuracy Plot** (`train&test accuracy.png`)
   - Comparison of training and test accuracy over 24 epochs
   - Shows convergence pattern
   - Identifies overfitting/underfitting trends
   - Demonstrates model generalization

---

## üöÄ Project Structure

```
S11/
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ dataloader11.py          # Data loading and Albumentations pipeline
‚îÇ   ‚îú‚îÄ‚îÄ lr_range_test.py         # Learning rate range finder
‚îÇ   ‚îú‚îÄ‚îÄ test11.py                # Model evaluation and testing
‚îÇ   ‚îú‚îÄ‚îÄ train11.py               # Training loop implementation
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ       ‚îú‚îÄ‚îÄ A11.py               # Main custom ResNet model (used in training)
‚îÇ       ‚îú‚îÄ‚îÄ model7.py            # Alternative: Depthwise + Dilated convolutions
‚îÇ       ‚îî‚îÄ‚îÄ resnet.py            # Alternative: Standard ResNet18
‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ cyclic LR plot.png      # One Cycle Policy visualization
‚îÇ   ‚îú‚îÄ‚îÄ LR plot.png              # LR range test results
‚îÇ   ‚îî‚îÄ‚îÄ train&test accuracy.png  # Training progress visualization
‚îú‚îÄ‚îÄ EVA_session11_assignment.ipynb  # Main Jupyter notebook
‚îî‚îÄ‚îÄ README.md                    # This documentation
```

---

## üíª Installation & Usage

### **Installation**

```bash
# Core dependencies
pip install torch torchvision albumentations tqdm matplotlib numpy opencv-python

# For GPU support (CUDA)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# Model summary tool
pip install torchsummary
```

### **Quick Start**

1. **Open the Jupyter Notebook:**
   ```bash
   jupyter notebook EVA_session11_assignment.ipynb
   ```

2. **Run cells sequentially:**
   - Data loading and visualization
   - Model architecture summary
   - Learning rate range test
   - Training with One Cycle Policy
   - Model evaluation and visualization

3. **Or use Python modules:**
   ```python
   from modules.models.A11 import Net
   from modules.dataloader11 import train_loader_Albumentation, test_loader
   from modules.train11 import train
   from modules.test11 import test
   from torch.optim.lr_scheduler import OneCycleLR
   import torch.optim as optim
   import torch.nn as nn
   import torch
   
   device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
   model = Net().to(device)
   
   optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.005)
   scheduler = OneCycleLR(optimizer, max_lr=0.04006, epochs=24, steps_per_epoch=1,
                          pct_start=0.208, anneal_strategy="linear", cycle_momentum=False,
                          base_momentum=0.85, max_momentum=0.95, div_factor=10.0, final_div_factor=1)
   loss_func = nn.CrossEntropyLoss()
   
   for epoch in range(24):
       train(model, device, train_loader_Albumentation, optimizer, loss_func, epoch)
       test(model, device, test_loader, loss_func, last_epoch=(epoch==23))
       scheduler.step()
   ```

---

## üîë Key Achievements Summary

‚úÖ **92.22% test accuracy** achieved in just 24 epochs  
‚úÖ **8 out of 10 classes** with 100% accuracy  
‚úÖ **Efficient training**: ~23 seconds per epoch  
‚úÖ **Minimal overfitting**: Only 1.67% gap between train (93.89%) and test (92.22%) accuracy  
‚úÖ **Comprehensive evaluation**: Class-wise analysis and misclassification tracking  
‚úÖ **Modern techniques**: One Cycle Policy, advanced augmentation, residual connections  

---

## üìö Technical Concepts Explained

### **Residual Connections**
Skip connections allow gradients to flow directly through the network, solving the vanishing gradient problem and enabling training of deeper architectures.

### **One Cycle Policy**
A learning rate scheduling technique that:
- Starts low, increases to maximum, then decreases
- Enables super-convergence (faster training with better results)
- Helps escape local minima and find better solutions

### **Data Augmentation Impact**
- **CutOut**: Forces model to learn robust features, not just specific patterns
- **RandomCrop**: Teaches model to recognize objects at different positions
- **HorizontalFlip**: Doubles effective dataset size, improves generalization

---

## üéØ Skills Demonstrated

- **Deep Learning**: Custom architecture design, optimization, hyperparameter tuning
- **Computer Vision**: Image classification, data augmentation, feature extraction
- **MLOps**: Training pipeline, model evaluation, performance tracking
- **Software Engineering**: Modular design, clean code, reusable components
- **Data Science**: Statistical analysis, visualization, result interpretation
- **Problem Solving**: Systematic approach to model improvement

---

## üìù Notes

- Model uses `log_softmax` in forward pass, compatible with `CrossEntropyLoss`
- Batch size 512 optimized for GPU memory and training speed
- Project designed for Google Colab but works on any PyTorch environment
- All visualizations saved in `images/` folder for easy reference
- Model parameters: ~6.5M (approximate, varies with architecture)

---

**Note**: This project is part of the EVA (Extreme Vision Analytics) program, demonstrating advanced deep learning techniques for image classification on the CIFAR-10 dataset.

